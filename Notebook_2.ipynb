{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Notebook_2.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMyhOeX1QlFMxdn2rbgd63P",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/leonardoLavagna/PyTorch-Notebooks/blob/main/Notebook_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PyTorch Notebook 2\n",
        "## Gradients, Optimizers and Loss Functions\n",
        "In this notebook we will introduce three fundamental concepts: gradients, optimizers and loss functions in Pytorch. "
      ],
      "metadata": {
        "id": "Ytlj2kmvh0hu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Gradients"
      ],
      "metadata": {
        "id": "bzXurH8OilGB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "-HbbwNttfIvT"
      },
      "outputs": [],
      "source": [
        "import torch"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We start by creating a random tensor, with the fundamental parameter `requires_grad` set to `True`, see: https://pytorch.org/docs/stable/generated/torch.Tensor.requires_grad.html. The main reason to specify this parameter has to do with how PyTorch calculates the gradients. In particular with this parameter se to `True` PyTorch will create a function that will be used in the back-propagation.\n",
        "\n",
        "Recall that by the chain rule, if we have $z=f(g(x))$ with $y=g(x)$, then the gradient (derivative) is calculated \"backwards\" since $z'=f'(g(x))g'(x)$. In practice we have $x\\to y=f(x)\\to z=g(y)$ for the \"forward pass\" and $\\frac{dz}{dy}\\to \\frac{dz}{dy}\\frac{dy}{dx}\\to z'$ for the \"backward pass\". "
      ],
      "metadata": {
        "id": "YfoZ6Xe0itlw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.randn(3, requires_grad=True)\n",
        "print(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zcp-D0CUisDk",
        "outputId": "10a48e82-b603-4047-e17b-9569f8f8ba37"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([-0.4491, -0.0873, -0.0493], requires_grad=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For example if we compute $y = 2x$ we see that the function `MulBackward` as been created and stored (`MulBackward` is for multiplication, if we compute $y=x+2$ an `AddBackward` function will be created, and so on...)."
      ],
      "metadata": {
        "id": "GlSyB190i-5H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# y = f(x) with f : x --> 2x\n",
        "# z = g(y) with g : y --> y+1\n",
        "y = 2*x\n",
        "z = y+1\n",
        "print(y)\n",
        "print(z)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tyr52kcIkDCh",
        "outputId": "da193586-a5d7-41b6-cc14-bab63013416b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([-0.8983, -0.1746, -0.0985], grad_fn=<MulBackward0>)\n",
            "tensor([0.1017, 0.8254, 0.9015], grad_fn=<AddBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example of gradient calculation\n",
        "# Note : we need to operate with scalars\n",
        "w = z.mean()\n",
        "print(f\"tensor w={w} with dimension {w.ndim}\")\n",
        "w.backward()\n",
        "gradient = x.grad\n",
        "print(f\"Gradient of w: {gradient}\") "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Frokzs-CkIDt",
        "outputId": "9053a236-7f24-451c-e3f1-fbf2cafb4d62"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor w=0.6095460653305054 with dimension 0\n",
            "Gradient of w: tensor([0.6667, 0.6667, 0.6667])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Reamrk .** It can happen that a mistake is made in the gradient calculation, say for example one writes by mistake `x.grad()` instead of `x.grad`. In this case if in the back-propagation process some values have been already stored, then a runtime error can occur. In this case it should suffice to restart the kernel, correct the mistake, and run again the cells. Another option can be to do not keep track of history (see below)."
      ],
      "metadata": {
        "id": "k3VXeT1BqF3Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# do not keep track of history\n",
        "# 1) x.requires_grad_(False)\n",
        "# 2) x.detach()\n",
        "# 3) with torch.no_grad()\n",
        "# e.g.\n",
        "x.requires_grad_(False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nkr5q1_cpxxX",
        "outputId": "163e535e-d3e2-4a6b-cafc-b2d202205087"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([-0.4491, -0.0873, -0.0493])"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loss functions and Optimizers\n",
        "Graidents play a central role in defining Machine Learning/Deep Learning models, in particulare when dealing with:\n",
        "\n",
        "\n",
        "*   Loss functions (minimization of the loss), see https://towardsdatascience.com/common-loss-functions-in-machine-learning-46af0ffc4d23\n",
        "*   Optimizers, see: https://towardsdatascience.com/optimizers-for-training-neural-network-59450d71caf6\n",
        "\n",
        "We will not discuss in detail these concepts, but we will frequently use them in this series of notebooks. "
      ],
      "metadata": {
        "id": "LthRJZYvwxsR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## A dummy model\n",
        "Let's see an important application of the concepts we've seen in practice. We will implement a dummy model in two different ways: manually using `numpy` and automatically using PyTorch. Here we see a classic Macine Learning/Deep Learning pipeline: \n",
        "\n",
        "\n",
        "1.   Get the data;\n",
        "2.   Define the model and choose an optimizer and a loss function;\n",
        "3.   Fit the data to the model and make inference, in particular:\n",
        "     *   Create a training loop;\n",
        "     *   Create a testing loop if necessary.\n",
        "\n",
        "Our dummy model consists of a simple vector $X$, a function $Y=f(X)=2X$ and we want to predict the value $f(5)$ without computing $y=2\\cdot 5=10$. "
      ],
      "metadata": {
        "id": "KNRXgCV_ravY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Get the data\n",
        "X = np.array([1,2,3,4])\n",
        "# Y = 2*X\n",
        "Y = np.array([2,4,6,8]) \n",
        "# weights\n",
        "W = 0.0"
      ],
      "metadata": {
        "id": "dbF7OQWKq-mO"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model definition\n",
        "def forward(x):\n",
        "    return W*x\n",
        "# loss function\n",
        "def loss(y,y_predicted):\n",
        "    return((y_predicted-y)**2).mean()\n",
        "# optimizer (gradient descent)\n",
        "# MSE = 1/N * (w*x-y)^2\n",
        "# MSE derivative = 1/N*2x(w*x-y)\n",
        "def gradient(x,y,y_predicted):\n",
        "    return np.dot(2*x,y_predicted-y).mean()\n",
        "\n",
        "print(f\"Prediction before training f(5)= {forward(5)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VYxkFFADuCFN",
        "outputId": "39d7dca9-5e7f-4916-ad04-3bc0467a1de5"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction before training f(5)= 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop\n",
        "# learning rate\n",
        "lr = 0.01\n",
        "# number of epochs\n",
        "n_iters = 10\n",
        "for epoch in range(n_iters):\n",
        "    # prediction\n",
        "    y_pred = forward(X)\n",
        "    # loss\n",
        "    l = loss(Y,y_pred)\n",
        "    # optimizer\n",
        "    dw = gradient(X,Y,y_pred)\n",
        "    # update\n",
        "    W -= lr*dw\n",
        "\n",
        "    # Print what's happening every epoch\n",
        "    if epoch % 1 == 0:\n",
        "        print(f\"epoch {epoch+1}: w = {w}, loss = {l}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VQ6uCYYeuXPk",
        "outputId": "9d490ab0-cb94-4ee5-8675-e48537534c22"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 1: w = 0.6095460653305054, loss = 30.0\n",
            "epoch 2: w = 0.6095460653305054, loss = 4.800000000000001\n",
            "epoch 3: w = 0.6095460653305054, loss = 0.7680000000000002\n",
            "epoch 4: w = 0.6095460653305054, loss = 0.12288000000000023\n",
            "epoch 5: w = 0.6095460653305054, loss = 0.019660800000000138\n",
            "epoch 6: w = 0.6095460653305054, loss = 0.0031457280000000165\n",
            "epoch 7: w = 0.6095460653305054, loss = 0.0005033164799999944\n",
            "epoch 8: w = 0.6095460653305054, loss = 8.053063680000391e-05\n",
            "epoch 9: w = 0.6095460653305054, loss = 1.2884901887999318e-05\n",
            "epoch 10: w = 0.6095460653305054, loss = 2.061584302080135e-06\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"prediction after training f(5)={forward(5)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P5zzHZauvn4E",
        "outputId": "0cb7719d-db8e-4053-a300-ee384c37b3d8"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prediction after training f(5)=9.998951424\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Same as before, but automatically with PyTorch\n",
        "# To carry out the model the following module is needed\n",
        "import torch.nn as nn\n",
        "\n",
        "# Data\n",
        "X = torch.tensor([[1],[2],[3],[4]], dtype=torch.float32)\n",
        "Y = torch.tensor([[2],[4],[6],[8]], dtype=torch.float32)\n",
        "\n",
        "# Model\n",
        "X_test = torch.tensor([5], dtype=torch.float32)\n",
        "samples , features = X.shape\n",
        "print(samples, features)\n",
        "\n",
        "input_size = features\n",
        "output_size = features\n",
        "model = nn.Linear(input_size, output_size)\n",
        "\n",
        "# Loss and optimizer (Stochastic Gradient Descent)\n",
        "loss = nn.MSELoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr = 0.01)\n",
        "\n",
        "print(f\"Prediction before training f(5)= {model(X_test).item()}\")\n",
        "\n",
        "# Training\n",
        "lr = 0.01\n",
        "n_iters = 10\n",
        "for epoch in range(n_iters):\n",
        "    # Prediction\n",
        "    y_pred = model(X)\n",
        "    # Loss\n",
        "    l = loss(Y,y_pred)\n",
        "    # gradients\n",
        "    l.backward() \n",
        "    # Update\n",
        "    optimizer.step()\n",
        "    optimizer.zero_grad()\n",
        "    if epoch % 1 == 0:\n",
        "        [w,b] = model.parameters()\n",
        "        print(f\"epoch {epoch+1}: w = {w[0][0].item()}, loss = {l}\")\n",
        "  \n",
        "print(f\"Prediction before training f(5)= {model(X_test).item()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PnIQ7fmhvt0J",
        "outputId": "23bbe7ad-7b20-419a-c9b1-4c2824256378"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4 1\n",
            "Prediction before training f(5)= 0.453784704208374\n",
            "epoch 1: w = 0.28903692960739136, loss = 25.857940673828125\n",
            "epoch 2: w = 0.5210843086242676, loss = 17.98900032043457\n",
            "epoch 3: w = 0.714539110660553, loss = 12.528627395629883\n",
            "epoch 4: w = 0.8758460879325867, loss = 8.73951530456543\n",
            "epoch 5: w = 1.010373592376709, loss = 6.11005163192749\n",
            "epoch 6: w = 1.1225935220718384, loss = 4.28524923324585\n",
            "epoch 7: w = 1.216230869293213, loss = 3.0187840461730957\n",
            "epoch 8: w = 1.2943885326385498, loss = 2.139739513397217\n",
            "epoch 9: w = 1.3596513271331787, loss = 1.5295201539993286\n",
            "epoch 10: w = 1.4141722917556763, loss = 1.105833649635315\n",
            "Prediction before training f(5)= 7.91689395904541\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercises\n",
        "\n",
        "\n",
        "1.   Create some data points using the following code."
      ],
      "metadata": {
        "id": "XWHF3mpsykjy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import datasets\n",
        "X_np, y_np = datasets.make_regression(n_samples=100, n_features=1, noise=20, random_state=1)"
      ],
      "metadata": {
        "id": "pHwFn3Ory2XW"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.   Convert `X_np` and `y_np` to tensors `X` and `y` with data type `float32` , and then create two variables `n_samples` and `n_features` with the dimensions of `X`."
      ],
      "metadata": {
        "id": "viF2lUIIy7hf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Solution of 2.\n",
        "X_np, y_np = datasets.make_regression(n_samples=100, n_features=1, noise=20, random_state=1)\n",
        "X = torch.from_numpy(X_np.astype(np.float32))\n",
        "y = torch.from_numpy(y_np.astype(np.float32))\n",
        "y = y.view(y.shape[0],1)\n",
        "\n",
        "n_samples, n_features = X.shape"
      ],
      "metadata": {
        "id": "Tvj3_sPLy6Qe"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.    Design a linear model (input, output size, foreward method) for the data. Then choose a suitable loss function, a suitable optimizer and a learning rate."
      ],
      "metadata": {
        "id": "BOSL-Jtp0kcs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Solution of 3.\n",
        "input_size = n_features\n",
        "output_size = 1\n",
        "model = nn.Linear(input_size, output_size)\n",
        "criterion = nn.MSELoss()\n",
        "lr = 0.01\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr =lr)"
      ],
      "metadata": {
        "id": "mwZNjFdY0UjH"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4.    Define a training loop (forward pass, backward pass, update) and plot the results."
      ],
      "metadata": {
        "id": "G4-JYt6G0_zu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Solution of 4.\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "n_epochs = 100\n",
        "for epoch in range(n_epochs):\n",
        "    # forward pass and loss\n",
        "    y_predicted = model(X)\n",
        "    loss = criterion(y_predicted, y)\n",
        "    \n",
        "    # backward pass\n",
        "    loss.backward()\n",
        "    \n",
        "    # update\n",
        "    optimizer.step()\n",
        "    \n",
        "    # reset at each iteration\n",
        "    optimizer.zero_grad()\n",
        "    \n",
        "    # print\n",
        "    #if (epoch+1) % 10 == 0:\n",
        "        #print(f\"epoch = {epoch+1}, loss = {loss.item()}\")\n",
        "\n",
        "# plot\n",
        "predicted = model(X).detach().numpy()\n",
        "plt.plot(X_np, y_np, \"ro\")\n",
        "plt.plot(X_np, predicted, \"b\")\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "gPp7Tv4o04p6",
        "outputId": "d4f5ea44-b5ea-4e7b-9d46-6cf3074da22e"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX8AAAD4CAYAAAAEhuazAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3df5RcZZ3n8fc3HRppRYd0MujkR3d0454JMy679Mk4Z9cfqyiBUYMyMmE6kRXZNoBndY/nrGD+GGfP9O4cZ0ePLgKGNRjsHgJHRcOKIHFn1DlHRhoXMMCiDaRDMghNGGAgSH599497K32r6t76eW/dqrqf1zl1uuupW1VP6sC3nn6e7/N9zN0REZFiWZR3B0REpPMU/EVECkjBX0SkgBT8RUQKSMFfRKSAFufdgUYtXbrUR0dH8+6GiEjPuPfee59x92Vxj/VM8B8dHWVmZibvboiI9Awzm0t6TNM+IiIFpOAvIlJACv4iIgWk4C8iUkAK/iIiBaTgLyJSaXoaRkdh0aLg5/R03j1KnYK/iEjU9DRMTMDcHLgHPycmOv8FkPEXkIK/iEjU1q1w6FB526FDQXundOALSMFfRCRq377m2rPQgS8gBX8RkahVq5prz0IHvoAU/EVEoiYnYWiovG1oKGjvlA58ASn4i4hEjY/Dtm0wMgJmwc9t24L2TunAF1DPFHYTEemY8fHOBvu494dgjn/fvmDEPzmZap808hcRyVNSSuf4OOzdC8ePBz9T/jLSyF9EJC+llM5SZk8ppRMy/8tDI38RkbzkuKdAwV9EJC857ilQ8BcRyUuOewoU/EVE8pLjngIFfxGRvOS4p0DZPiIiecppT0EqI38z225mT5vZnkjb58zsgJndF97Oizx2lZnNmtkjZnZOGn0QEWlJvdLJfVrbP62R/9eBq4EbK9q/6O7/I9pgZmuBjcAZwO8Au83sze5+LKW+iIg0pl6efY55+FlLZeTv7j8Gnm3w8g3ATnd/xd0fB2aBdWn0Q0SkKfXy7Luhtn9Gsl7w/YSZPRBOC50Wti0Hnohcsz9sq2JmE2Y2Y2Yz8/PzGXdVRPpW0tRNvTz7HPPwv/71YA34Qx/K5vWzDP7XAm8CzgSeBP662Rdw923uPubuY8uWLUu7fyJSBLVOxaqXZ59DHv6NNwZB/6MfDe6femo275NZ8Hf3p9z9mLsfB65nYWrnALAycumKsE1EJH21pm7q5dl3MA9/ejoI+hdfHNxftCio57ZjR+pvFbx+Ni8LZvaGyN0PAqVMoF3ARjM72cxWA2uAn2XVDxEpuFpTN/Xy7DuQh79hQ/DSmzYttD3+OBw7FrxdVszd238Rs5uAdwJLgaeAPwvvnwk4sBf4uLs/GV6/FbgEOAp8yt2/X+89xsbGfGZmpu2+ikjBjI4GUz2VRkaCoXVOLrgAvv3t8rZHH4U3vjG99zCze919LO6xVFI93f2imOav1bh+EujgmWgiUliTk+XpmtD5YxkjNm6Em28ub7vjDjinwzueVN5BRPpbNxzLSDCtY1Ye+G+/PViD7nTgBwV/ESmCRk7Fymgn7yWXBEE/+nK33RYE/XPPTeUtWqLaPiIiGezknZiA668vb7v1Vjj//Db6mSKN/EVEUtzJe8UVwUg/Gvi/+c1gpN8tgR8U/EVEUtnJ+8lPBkH/mmsW2m6+OQj6F1zQZv8yoOAvItLGTt5PfzoI+l/+8kLb9HQQ9C+8MKX+ZUDBX0Ra1y/ljlvYyfuZzwRB/wtfWGi78cYg6P/pn2bUzxQp+ItIa2rVzOk1TaSDbt0aXPL5zy+03XBD8BFs3tzBPrcplR2+naAdviJdYHo6iH779gWj/WMxx3DkvHM2K5/7HPz5n5e3XX89XHppLt1pSK0dvhr5i0hjKkf6cYEf0i133AXTSmefHYz0o4H/uuuCj6CbA389yvMXkcbEpUPGSavccc6naJ13Hny/ourY1VcHqZz9QCN/EWlMIyP6NGvm5HSK1umnByP9aOC/6KJgpN8vgR8U/EWkUUkj+oGBbGrmdPgULbPg9vTTC23veU8Q9P/mbzJ5y1wp+ItIY5LSIXfsqF0zp1UdOkWrFPSjXvvaIOj/4AepvlVXUfAXkcZ0ujpmxqdoxQV9CIL+88+n8hZdTcFfRBrXSHXMNN+r1S+bGllCtYJ+j2S+p0J5/iLSXyqzhACGhrBDL8Ve3iMhsCWZ5/mb2XYze9rM9kTalpjZXWb2q/DnaWG7mdmXzWzWzB4ws3+TRh9EJGWdyLHP4j0qsoQMjw38RRvpV0pr2ufrwPqKtiuBH7r7GuCH4X2AcwkObV8DTADXptQHEUlLJ0o3xL3H5s1w+eXtvW6YDWQ4RnV0L3rQL0kl+Lv7j4FnK5o3ADvC33cA50fab/TA3cBvmdkb0uiHiKSkEzn2ce/hHmyfbeNLxvx4fNAfGVXQj8hywfd0d38y/P3XwOnh78uBJyLX7Q/bqpjZhJnNmNnM/Px8dj0VkXKdyLFPei334MDbJqeBEhdyMXzo1bkd2N6tOpLt48GqctPfue6+zd3H3H1s2bJlGfRMRGJ1Ise+3ms1ONWUGPRHRnFblNuB7d0uy+D/VGk6J/xZ2jd3AFgZuW5F2CYi3SLjHPsT7xEXtaNqTDXVTdnsVEpqj8oy+O8CLg5/vxj4bqT9I2HWz1uB5yPTQyLSDTqxoWt8HLZsqf8FUDE9pDz9dKSS529mNwHvBJYCTwF/BnwHuAVYBcwBF7r7s2ZmwNUE2UGHgI+6e90EfuX5i/Sp0hkBc3Pxj4fnA5xyCvzmN9UPK+Anq5Xnr01eItIdEjZnncazPHfo5KrLeyR05apW8Fc9fxHpDqUppfCkMPPjwdxABQX9dKi2j4jkp3KHL2Bze4PAX0Fz+ulS8Bcpii44ErGqP5Edvja3F9tUvaCsoJ8NTfuIFEHORyLGCnf4xu3GBQX8rGnkL1IEaZdrSOGvCJvbG1+GwRYp8HeAgr9IEaRZrqHNgmw1yzBgsGRJ832Spin4ixRBmuUaWizIVjfoS0cp+IsUQZrlGmoVZIuZRkoM+rYoPug/W1kgWLKg4C9SBPXKNTQyh1+6ptaE/NzciefXLcPQoQPaJZ52+IoUXcLO2qovh8prEgzxEi8zVNVeFWoaeV9pS+bHOIpID2skEyjumgqv50kMrwr8iXn6nSgeJ4k08hcpukWL4qOzWVASudY1wJuY5THeVNXutmjh+ZILjfxFJFkjc+8x15TOyK0M/CeydzR339UU/EWKrpFMoMg1r+X5+M1Z0ZTNtA9+kdQp+IsUXeXc+/AwnHJKsHGrlPkzPs7r/J8wnH/mtWVPdww/aTB4nubue4aCv4gEgXrvXvjGN+Dll+HgwRO7d1dufgdm8MLLg2VPKTsj94Yb4JlndGxiD1HwF+lVrdbXqfW8SFbPW7gfw9nvK8qefvy4zsjtB5kHfzPba2a/MLP7zGwmbFtiZneZ2a/Cn6dl3Q+Rjsq6fHJcfZ2JifrvU+95+/bxr7gPw/kFbyl7aino1ztyV3pD5qmeZrYXGHP3ZyJtnweedfe/NLMrgdPc/TO1XkepntIzOrF5aXQ0/szb8LzbVp739lV7+clPqh86ygADIytrv650pW5M9dwA7Ah/3wGcn1M/RNKXdvnkOK1W6Yx5/P3swuaqA/8RFuMYA0OvUuZOH+pE8HfgB2Z2r5mFp0dwurs/Gf7+a+D0uCea2YSZzZjZzPz8fAe6KpKCpABcqnuTxlRQs3VxYuryjDOF4fxv3l926W9uuAkfGWWxHVfmTj9z90xvwPLw528D9wNvB56ruOaf6r3OWWed5SI9YWSkVNGg/GZWfn9oyH1qqrX3mJoKnt/I61Vc+3Guje3eSy+19a+WLgTMeEJMzXzk7+4Hwp9PA7cC64CnzOwNAOHPp7Puh0jHxG2aMqsuj3DoEGza1NpfAaXc/OHhhbZTTom/NpyGuoKrMZyvsqXs4Reuvxn36i5Lf8s0+JvZq83s1NLvwHuBPcAu4OLwsouB72bZD5GOiitYVq8McmWmTqPZQi+/vPD7wYOxGT+fnfs4hnMNV5S1P8sS3OHUS/+kuX+f9IekPwnSuAFvJJjquR94ENgatg8DPwR+BewGltR7LU37SE9LmgqK3kZGgmvjpnTM3C+7rLHXDF/nL/4i/uFf89vl79eOqangdcyCn61OY0kmqDHtk/mcf1o3BX/paXEBPW5NwL32mkE0uFauIYS3Sa6Kffo+VizcaWe9oda/KY3XldTUCv7a4SvSCdGpoCSlTJ16xyQmnKj1Jf4ThrOV/1bWPsu/wN99NitHBtKtvdOJlFbJzOK8OyDS16ang2C4b18Q3Ev58nGbwEqPrVoVvxELFtYHIs/9GpdwKV+ruvR+3sJb+EVw5/88FtTtSTNls9W9BtIVNPIXyUpSKQWofYLV5GRyDYWBgROBf4pxDK8K/P/AOhxbCPyQeLh6W3QGb09T8BfJSq1pkWgVTagqn8yWLfFfAMeOcRvvw3A2M1X20N/9XVBpcx33xPcn7RF5I+cASNdS8BfJSr1pkVpF1q65JvhiiOTx/5B3YTgf4Layl7uN9+Ejo7zjHdT+qyHtEbnO4O1pCv4iWak3LVJvwTQMoj/h32E4Z/PDskt38ic4xvuG/rZ8tB23WyurEXnpLxiVdu45WvAVycL0NLz4YnV7NAjX+cvgZ//1Dv7g4DNVD3+DTWxiOhhtrxoJXm98PL6aKAR/PXzpSwrMUkbBXyRtjQbhJUuCXbkV7n/9OZxpAOvL2q9lC1v4anAnrnRz3F8SAK95jQK/VFHwF0lbI0F4ehqef77s4Yf4Xc7gIXiy/GmTfJbP8t8rGmOmcJR6KU3QnL9I2hoJwlu3wtGjADzGagwPAn/Ep079Go5VB/7h4fiRvFIvpQkK/iJpSwq2S5YsFGubm2M/yzGcN/FY2WWb+Abu8MVrXxWfSvmlL8W/vlIvpQkK/iJpiwvCg4PwwgswN8e8D2M4K9lfdsm72Y1jfIOPBA3NplIq9VKakPkZvmnRGb7SUyrLOrz4Is8dPMppPFd16e/yEA9xxkLD8DA8U53lI9KsWmf4asFXJAvj4ydG3C+9FKz1VhrlcR7njeWNg4PJ0zoiKdK0j0hGfvObYPalMvC/mhdxLAj8w8Pl0zTbt2uaRjpCwV+kUqOnaCU4ciSI5XGnKjrGi5wa3Ckt3pZ2yE5OBlNFaRzwLlKHgr9IVK16O3UcOxYE/cHB6sfcwaemkxdj23hfkVbkFvzNbL2ZPWJms2Z2ZV79ECnTwgEl7kE8Xxyzgua2CB8ZXajWmVQHJ4uDUdr8C0b6Wy7B38wGgK8A5wJrgYvMbG0efREp08Qu2VLQXxTzf5EPvRrHykfxl1+eHIzT3p2rvySkjrxG/uuAWXd/zN0PAzuBDTn1RYouOkKOi+RQtXErMeh7UFM/dhR/3XXJwTjt3bk6YlHqyCv4LweeiNzfH7aVMbMJM5sxs5n5+fmOdU4KpHKEfOxY9TWRXbJm8eXySyeYA7XP4I2KBuO0d+eqzo/U0dULvu6+zd3H3H1s2bJleXdHelG9ee+kImwD5Yed26bx+kG/pJnReikYp707V3V+pI68gv8BYGXk/oqwTSQ9jcx7J42Ejx+H48exub3YpuoA7COjQfZOnLhRfKdO16rVB9X5kSh37/iNYGfxY8BqYBC4Hzij1nPOOussF2nKyEhpYF5+Gxmpe03c04L/WyJ3hobcp6bi33tqKnhts+DnZZcF1yc9f2qq9uOtqOxDO68lPQmY8aQ4nPRA1jfgPOCXwKPA1nrXK/hL08ziI7jZwjVTU+6Dg/WDftIXSenLpJHAWisYN/JFJdKkWsFfhd2kf42OBlM9lSpPwVq6FIs5LhEi8/mLFsVM7kcMDbU3R5/0+mbBFJRIC2oVduvqBV+RtjQw721GbOB3DLfI/x715ubbTaPUAq10mIK/dL9Wd6qWMmiGhxfawoI7iSmbWLA5C8oDb9wXSaV20ii1QCsdpuAv3S2Nnaovv3ziVzv4THz2TmlHbkll4I2mYiZpZ5Sug1ikwxT8pbs1slO11l8G4fMtHNNXKq2sxgZeKH9dCNYKpqayGaXXqv0jkrakleBuuynbp6DqZezUSZFMzN4xq519Uy/1UmmU0gPoxlTPZm8K/n0oKYBG2wcGaqdAtpqnb1aW4lkV3IeHa7+vSA+oFfw17SP5SJrLv/zypmrtVC6yJk7vRBdyIXjtw4fLLypNJ01Pw8GD8f1OWtRV+WTpMQr+ko+kufxt2xqqtXNiPjxcZE0M+lPT+ODJjfdrbg4uvjj58bhFXZVPlh6kTV6Sj3qbpiolbHZKKpnjU+HhKUkbvWq9T61+TU1VL8Q2uplMpMO0yUu6T1Ja5MBAQ9cn5umXCq6VAnSzufe1Av/wcHwGjsonSw9S8Jd8JG1qmpiomUZZc3PW0KuD66IBOq0dsqXD1uNod670IAV/yUfSpqZrroltT6ynH13IjSux0MjOXAiuie4EjhoYqL3hSrtzpRclpQF1202pngVRkf5ZM0+/XsXOhNf0qanktlbLKivvX7oQNVI9F+f95SNyQilrJtyRS8wa6okp+dFV8YuscVMt4+Plo/bp6eAvhH37gusrp4o++cmFVM+wFlBdle8h0uU07SPdY+tW7NBLyXn6I6ML6ZOtTrU0kpYZqQXEwYNK25S+pFRP6QqJKZtUPDA4CNu3B6PseiP4OPXSMpW2KX2kVqqngr/kquGgHzU8DM/EH75SV71DU3SoivSRXPL8zexzZnbAzO4Lb+dFHrvKzGbN7BEzOyerPkj3SkzZtEW1Az8kl15oRL20TKVtSkFkPef/RXc/M7zdDmBma4GNwBnAeuAaM0vY2SP9pmbQHxmFd70r+c+BNNRbK1DaphREHgu+G4Cd7v6Kuz8OzALrcuiHNKPNwmWJQb90iEpp8fWnP4UtW2ofmpKUj9+Ieoem6FAVKYisg/8nzOwBM9tuZqeFbcuBJyLX7A/bqpjZhJnNmNnM/Px8xl2VRG0ULksM+h6UYogt7nb77QuHppx0UvWTL7ywpX8G09OwdCls2hT8G5YsiV8k1qEqUgBtBX8z221me2JuG4BrgTcBZwJPAn/d7Ou7+zZ3H3P3sWXLlrXTVWlHI6dpVagZ9EvrqfVq4oyPw6WXVr/Qjh3Np15OT8NHP1q+XnDwIFxyidI4pZDaCv7ufra7/17M7bvu/pS7H3P348D1LEztHABWRl5mRdgm3aqJwmV1C65FJS2iLlq0ML10yy3V2Td1vnhibd0KR45Utx8+3PxrifSBLLN93hC5+0FgT/j7LmCjmZ1sZquBNcDPsuqHpKCBDJiaBdewYJqlcpSdVHfn2LGF6aVmD1VJUut6Vd+UAspyzv/zZvYLM3sA+PfAfwZw9weBW4CHgDuAK9w95rgm6Ro1MmASg/7w0uqUzcOHg9IJJZWLq0nlnOM0m3pZ63qlcUoBZVbbx90313hsElDuXK8oLXhGdtPa3F7YVH3piRkaSxix18rRjzuyMU4rqZeTk8Gcf+XUz+Cg0jilkFTbRxoTZsCYHw8Cf4WyhdxGVWYR1TI83F7q5fg43HBDeZro8PBCqQiRglFVT2lIYhmGpJg9PBw/yo8G37gsoiSveU3rJR1KVHlT5ASN/KWmhlI2S6IbwWDhZ9TBgwubxJpZaNWirEiqFPwl1urVTQR9qJ7COXgQFi9eGOlHX6y0SWzJksY7pEVZkVQp+EuZ3//9IE5XVi+uO6cfN4Vz+HAwXTMyEp+rD9VZRIOD1bt6VVtHJHUK/gLAunVB0N+zp7z9RJ7+0qW1d8LW2giW9Nizz1bX0dm+PViYVW0dkUypnn/Bvf3t8JOfVLfHllUeGkoOxLUOQQEdkCKSg1zq+Ut3e+97g4F1ZeCvWU+/VlmFWqWQVSZZpOso+BfMBz4QBP277ipvPzGnX29hNWkKp1YpZJVJFuk6mvYpiD/+Y/jWt6rbY9M1JyaS8+81VSPSMzTtU2CbNweD7crAn5i9Uxqlxx2YYgbnnVfdLiI9R8G/T33sY0Gsnpoqb2+oDMP4eLCb9rLLyvPz3VurpS8iXUfBv8/85V8G8Xr79vL2lmrv3H57OrX0RaTrqLZPn/jCF+DTn65ub2tJp4lDXESktyj497hvfxsuuKC6PZV1/FWr4vPzVWpBpOdp2qdH7doVTO9UBv6WpneSTE4G5RaiVP9epC9o5N9jbr8d/uiPqtszy9itfOEeSQ0WkdraGvmb2YfN7EEzO25mYxWPXWVms2b2iJmdE2lfH7bNmtmV7bx/kdx5ZzDSrwz8VSP9aFnlUunkVsUden7kiBZ8RfpAuyP/PcCHgK9GG81sLbAROAP4HWC3mb05fPgrwHuA/cA9ZrbL3R9qsx99a/dueM97qttrllUubdAqlU6G1nbTasFXpG+1NfJ394fd/ZGYhzYAO939FXd/HJgF1oW3WXd/zN0PAzvDa6XCj34UjPQrA3/NOf24ssrtpGYmLexqwVek52W14LsceCJyf3/YltQey8wmzGzGzGbm5+cz6Wi3+fu/D4L+O99Z3t7QQm7aI3UVZBPpW3WDv5ntNrM9MbfMR+zuvs3dx9x9bNmyZVm/Xa7uvjsI+m97W3l7U9k7aY/UVZBNpG/VnfN397NbeN0DwMrI/RVhGzXaC+mee4KDVCq1lFQzOVldlK3dkboOPRfpS1lN++wCNprZyWa2GlgD/Ay4B1hjZqvNbJBgUXhXRn3oaj//eTCYrgz8beXpa6QuIg1qK9vHzD4I/E9gGfA9M7vP3c9x9wfN7BbgIeAocIW7Hwuf8wngTmAA2O7uD7b1L+gx998PZ55Z3Z5a+rxG6iLSANXz75A9e4LD0Sv1yMcvIj2oVj1/7fDN2MMPw9q11e3Hj5dXSxYR6STV9snIL38ZBPfKwH/8eDDaTzXwp7mrV0QKQSP/lM3Owpo11e2ZjfTT3tUrIoWgkX9KHn88CO6VgT+TkX5U2rt6RaQQNPJv0759QUZlpY7N6av+joi0QCP/Fu3fv5BKH3XsWMYj/UqqvyMiLVDwb9I//mMQ2FeuLG8/ejQI+os6/Ymq/o6ItEDBv0HPPRcE/eUVZehKQX9gIJ9+aVeviLRCc/51vPACvO511e1HjsDibvn0tKtXRJqkkX+CV16B9763OvAfORKM9Lsm8IuItEDBv8Lhw3DuufCqV8FddwVtJ52koC8i/UXBP3T4cHA+7sknwx13BG0f+Ugwp3/4sIK+iPSXwoe0I0fgggvgttsW2sbHYceOHBdxRUQyVtjgf+QIXHghfOc7C20bN8LUlIK+iPS/wgX/o0eDIP+tby20XXhhUCJHUzsiUhSFCXdHjwbTObfcstB2wQWwc6eCvogUT9+HvWPHYPNmuOmmhbbzzw++BE46Kb9+iYjkqa1sHzP7sJk9aGbHzWws0j5qZi+b2X3h7brIY2eZ2S/MbNbMvmyWbRWcxYsXAv/73x9k7tx6qwK/iBRbu6mee4APAT+OeexRdz8zvG2JtF8L/EeCQ93XAOvb7ENN114LGzYEm7Z27VLQFxGBNoO/uz/s7o80er2ZvQF4rbvf7cHhwTcC57fTh3q2bAkyegYHs3wXEZHekuUmr9Vm9n/N7Edm9rawbTmwP3LN/rAtlplNmNmMmc3Mz89n2FURkWKpu+BrZruB18c8tNXdv5vwtCeBVe5+0MzOAr5jZmc02zl33wZsAxgbG/Nmny8iIvHqBn93P7vZF3X3V4BXwt/vNbNHgTcDB4AVkUtXhG0iItJBmUz7mNkyMxsIf38jwcLuY+7+JPCCmb01zPL5CJD014OIiGSk3VTPD5rZfuAPge+Z2Z3hQ28HHjCz+4BvAlvc/dnwscuB/wXMAo8C32+nDyIi0jwLkm6639jYmM/MzOTdDRGRnmFm97r7WNxjKuksIlJACv4iIgWk4C8iUkAK/iIiBaTgLyJSQAr+IiIFpOAvIlJACv4iIgWk4F/L9DSMjsKiRcHP6em8eyQikoq+P8axZdPTMDEBhw4F9+fmgvsQHAYsItLDNPJPsnXrQuAvOXQoaBcR6XEK/kn27WuuXUSkhyj4J1m1qrl2EZEe0t/Bv50F28lJGBoqbxsaCtpFRHpc/wb/0oLt3By4LyzYNvoFMD4O27bByAiYBT+3bdNir4j0hf6t5z86GgT8SiMjsHdvWt0SEelaxaznrwVbEZFE7R7j+Fdm9v/M7AEzu9XMfivy2FVmNmtmj5jZOZH29WHbrJld2c7715T2gq02fIlIH2l35H8X8Hvu/hbgl8BVAGa2FtgInAGsB64xs4HwUPevAOcCa4GLwmvTl+aCbbvrByIiXaat4O/uP3D3o+Hdu4EV4e8bgJ3u/oq7P05wWPu68Dbr7o+5+2FgZ3ht+tJcsNWGLxHpM2mWd7gEuDn8fTnBl0HJ/rAN4ImK9j9IekEzmwAmAFa1Ml0zPp5Odo7WD0Skz9Qd+ZvZbjPbE3PbELlmK3AUSHUexN23ufuYu48tW7YszZdujjZ8iUifqTvyd/ezaz1uZv8BeB/wbl/IGz0ArIxctiJso0Z795qcLC/yBtrwJSI9rd1sn/XAfwE+4O7RSfFdwEYzO9nMVgNrgJ8B9wBrzGy1mQ0SLArvaqcPHaENXyLSZ9qd878aOBm4y8wA7nb3Le7+oJndAjxEMB10hbsfAzCzTwB3AgPAdnd/sM0+dEZa6wciIl2gf3f4iogUXDF3+IqISCIFfxGRAlLwFxEpIAV/EZEC6pkFXzObB2JqNOdiKfBM3p3oIvo8yunzKKfPo1wnP48Rd4/dIdszwb+bmNlM0gp6EenzKKfPo5w+j3Ld8nlo2kdEpIAU/EVECkjBvzXb8u5Al9HnUU6fRzl9HuW64vPQnL+ISAFp5C8iUkAK/iIiBaTg36Jah9cXkZl92MweNLPjZpZ7GlsezGy9mT1iZrNmdmXe/cmbmW03s6fNbE/efcmbma00s781s4fC/08+mXefFPxbF3t4fYHtAT4E/DjvjuTBzAaArwDnAmuBi8xsbb69yt3XgfV5d6JLHAU+7e5rgbcCV+T934eCf4tqHF5fSO7+sLs/knc/crQOmHX3x9z9MLAT2FDnOX3N3d0DtBMAAAEySURBVH8MPJt3P7qBuz/p7j8Pf/9n4GEWzjXPhYJ/Oi4Bvp93JyRXy4EnIvf3k/P/3NKdzGwU+NfAP+TZj3ZP8uprZrYbeH3MQ1vd/bvhNZkcXt+NGvk8RCSZmb0G+BbwKXd/Ic++KPjX0OLh9X2r3udRcAeAlZH7K8I2EQDM7CSCwD/t7t/Ouz+a9mlRjcPrpZjuAdaY2WozGwQ2Arty7pN0CQsOOf8a8LC7fyHv/oCCfzuuBk4lOLz+PjO7Lu8O5cnMPmhm+4E/BL5nZnfm3adOChf/PwHcSbCYd4u7P5hvr/JlZjcBPwX+pZntN7OP5d2nHP1bYDPwrjBe3Gdm5+XZIZV3EBEpII38RUQKSMFfRKSAFPxFRApIwV9EpIAU/EVECkjBX0SkgBT8RUQK6P8DSSM/1zWujm0AAAAASUVORK5CYII=\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5.   What is the difference between the technique developed in the previous points and the well known linear regression?"
      ],
      "metadata": {
        "id": "bwk9MMYh1mSr"
      }
    }
  ]
}